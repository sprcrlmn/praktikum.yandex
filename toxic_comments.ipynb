{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Содержание",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "302.391px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "8fcd5a04-40fb-4260-89f9-ef0366c38900.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knFKYHx_JiMF"
      },
      "source": [
        "# Проект для «Викишоп»"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MFPFqhcJiMF"
      },
      "source": [
        "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
        "\n",
        "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
        "\n",
        "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
        "\n",
        "**Инструкция по выполнению проекта**\n",
        "\n",
        "1. Загрузите и подготовьте данные.\n",
        "2. Обучите разные модели. \n",
        "3. Сделайте выводы.\n",
        "\n",
        "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
        "\n",
        "**Описание данных**\n",
        "\n",
        "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpDbxUuSJiMG"
      },
      "source": [
        "## Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGnVYsfJiMH",
        "outputId": "b78b5dcd-6ef0-46fc-b0f1-e2f1f47b8d3b"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdAJ9dFgJiMJ",
        "outputId": "16f565f5-9168-4fd4-cdfc-c1aaf9699c47"
      },
      "source": [
        "toxic_comments = pd.read_csv('/datasets/toxic_comments.csv')\n",
        "toxic_comments.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  toxic\n",
              "0  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  D'aww! He matches this background colour I'm s...      0\n",
              "2  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  You, sir, are my hero. Any chance you remember...      0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm1BKUeRJiMK",
        "outputId": "ad89672e-2cf1-4658-a28d-2ed23be8e1d0"
      },
      "source": [
        "toxic_comments.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 2 columns):\n",
            "text     159571 non-null object\n",
            "toxic    159571 non-null int64\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytxfQMSFJiML",
        "outputId": "a7a710dd-1d15-4168-cd63-14240e2ada71"
      },
      "source": [
        "display(toxic_comments['toxic'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    143346\n",
              "1     16225\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrK5cGeWJiML"
      },
      "source": [
        "**Вывод**  \n",
        "Классы несбалансированы. Необходимо учитывать это при обучении моделей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TThaxlfJiMM"
      },
      "source": [
        "Напишем функцию для лемматизации и очистки текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WIOZCWJiJiMM",
        "outputId": "b178118e-9462-4cef-c1e8-c14e1fa81bca"
      },
      "source": [
        "\"\"\"# функция лемматизации и удаления лишних символов\n",
        "m = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = text.lower()\n",
        "    lemm_text = \"\".join(m.lemmatize(text))\n",
        "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', lemm_text) \n",
        "    return \" \".join(cleared_text.split())\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# функция лемматизации и удаления лишних символов\\nm = WordNetLemmatizer()\\n\\ndef lemmatize_text(text):\\n    text = text.lower()\\n    lemm_text = \"\".join(m.lemmatize(text))\\n    cleared_text = re.sub(r\\'[^a-zA-Z]\\', \\' \\', lemm_text) \\n    return \" \".join(cleared_text.split())\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mckdq4skJiMN"
      },
      "source": [
        "# функция лемматизации и удаления лишних символов\n",
        "\n",
        "def clear_text(text):\n",
        "    re_list = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
        "    re_list = re_list.split()\n",
        "    re_list = \" \".join(re_list)\n",
        "    return re_list\n",
        "\n",
        "m = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    word_list = nltk.word_tokenize(text)\n",
        "    \n",
        "    return ' '.join([m.lemmatize(w) for w in word_list])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-yaGiZ0JiMN"
      },
      "source": [
        "toxic_comments['text'] = toxic_comments['text'].apply(clear_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WId5YlvIJiMO"
      },
      "source": [
        "toxic_comments['lemm_text'] = toxic_comments['text'].apply(lemmatize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUCoOumYJiMO",
        "outputId": "e920b310-6656-4099-d595-7768eb0723a0"
      },
      "source": [
        "toxic_comments"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>lemm_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my userna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>D'aww He matches this background colour I'm se...</td>\n",
              "      <td>0</td>\n",
              "      <td>D'aww He match this background colour I 'm see...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>Hey man I'm really not trying to edit war It's...</td>\n",
              "      <td>0</td>\n",
              "      <td>Hey man I 'm really not trying to edit war It ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>More I can't make any real suggestions on impr...</td>\n",
              "      <td>0</td>\n",
              "      <td>More I ca n't make any real suggestion on impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>You sir are my hero Any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>You sir are my hero Any chance you remember wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159566</td>\n",
              "      <td>And for the second time of asking when your vi...</td>\n",
              "      <td>0</td>\n",
              "      <td>And for the second time of asking when your vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159567</td>\n",
              "      <td>You should be ashamed of yourself That is a ho...</td>\n",
              "      <td>0</td>\n",
              "      <td>You should be ashamed of yourself That is a ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159568</td>\n",
              "      <td>Spitzer Umm theres no actual article for prost...</td>\n",
              "      <td>0</td>\n",
              "      <td>Spitzer Umm there no actual article for prosti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159569</td>\n",
              "      <td>And it looks like it was actually you who put ...</td>\n",
              "      <td>0</td>\n",
              "      <td>And it look like it wa actually you who put on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159570</td>\n",
              "      <td>And I really don't think you understand I came...</td>\n",
              "      <td>0</td>\n",
              "      <td>And I really do n't think you understand I cam...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  toxic  \\\n",
              "0       Explanation Why the edits made under my userna...      0   \n",
              "1       D'aww He matches this background colour I'm se...      0   \n",
              "2       Hey man I'm really not trying to edit war It's...      0   \n",
              "3       More I can't make any real suggestions on impr...      0   \n",
              "4       You sir are my hero Any chance you remember wh...      0   \n",
              "...                                                   ...    ...   \n",
              "159566  And for the second time of asking when your vi...      0   \n",
              "159567  You should be ashamed of yourself That is a ho...      0   \n",
              "159568  Spitzer Umm theres no actual article for prost...      0   \n",
              "159569  And it looks like it was actually you who put ...      0   \n",
              "159570  And I really don't think you understand I came...      0   \n",
              "\n",
              "                                                lemm_text  \n",
              "0       Explanation Why the edits made under my userna...  \n",
              "1       D'aww He match this background colour I 'm see...  \n",
              "2       Hey man I 'm really not trying to edit war It ...  \n",
              "3       More I ca n't make any real suggestion on impr...  \n",
              "4       You sir are my hero Any chance you remember wh...  \n",
              "...                                                   ...  \n",
              "159566  And for the second time of asking when your vi...  \n",
              "159567  You should be ashamed of yourself That is a ho...  \n",
              "159568  Spitzer Umm there no actual article for prosti...  \n",
              "159569  And it look like it wa actually you who put on...  \n",
              "159570  And I really do n't think you understand I cam...  \n",
              "\n",
              "[159571 rows x 3 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQMiOd5LJiMP"
      },
      "source": [
        "toxic_comments = toxic_comments.drop(['text'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWO1VscrJiMP"
      },
      "source": [
        "Создадим корпус из лематизированных и очищеных тексов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ono5ylnYJiMP"
      },
      "source": [
        "corpus = toxic_comments['lemm_text'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yHzG6QvJiMQ"
      },
      "source": [
        "Разобьем выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIA0_lXzJiMQ"
      },
      "source": [
        "features = corpus\n",
        "target = toxic_comments['toxic'].values\n",
        "\n",
        "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2, random_state=12345)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izPEUwRqJiMQ"
      },
      "source": [
        "stopwordss = set(stopwords.words('english'))\n",
        "\n",
        "count_tf_idf = TfidfVectorizer(stop_words = stopwordss)\n",
        "train_features = count_tf_idf.fit_transform(train_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRWUajQcJiMQ"
      },
      "source": [
        "test_features = count_tf_idf.transform(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ME-Kf_JiMQ"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41mHd_MhJiMQ"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eoC-5wOJiMR",
        "outputId": "47d3ae2d-9dba-4127-cc1d-e4704daa135a"
      },
      "source": [
        "lr_model = LogisticRegression()\n",
        "hyperparams = [{'C':[10],   # так же подбирал [0.1, 1, 3]\n",
        "                'class_weight':['balanced']}]\n",
        "clf = GridSearchCV(lr_model, hyperparams, scoring='f1',cv=3)\n",
        "clf.fit(train_features, train_target)\n",
        "print(\"Лучшие параметры модели:\")\n",
        "print()\n",
        "LR_best_params = clf.best_params_\n",
        "print(LR_best_params)\n",
        "print()\n",
        "print('F1:', clf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лучшие параметры модели:\n",
            "\n",
            "{'C': 10, 'class_weight': 'balanced'}\n",
            "\n",
            "F1: 0.762750967078337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn9Z7T9mJiMR"
      },
      "source": [
        "### CatBoostClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhayLJMdJiMR",
        "outputId": "d3ad9807-208d-43f1-bad4-65eac0aa9b7a"
      },
      "source": [
        "train_featurescat =  train_features.toarray() \n",
        "valid_featurescat =  valid_features.toarray() \n",
        "test_featurescat = test_features.toarray() \n",
        "cat_model = CatBoostClassifier(eval_metric=\"F1\", \n",
        "                                   iterations=100, \n",
        "                                   max_depth=6, \n",
        "                                   learning_rate=0.9, \n",
        "                                   random_state=43)\n",
        "cat_model.fit(train_featurescat, train_target, verbose=20)\n",
        "print(\"Лучшие параметры модели:\")\n",
        "print()\n",
        "CAT_best_params = clf.best_params_\n",
        "print(CAT_best_params)\n",
        "print()\n",
        "print('F1:', clf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ntrain_featurescat =  train_features.toarray() \\nvalid_featurescat =  valid_features.toarray() \\ntest_featurescat = test_features.toarray() \\ncat_model = CatBoostClassifier(eval_metric=\"F1\", \\n                                   iterations=100, \\n                                   max_depth=6, \\n                                   learning_rate=0.9, \\n                                   random_state=43)\\ncat_model.fit(train_featurescat, train_target, verbose=20)\\nprint(\"Лучшие параметры модели:\")\\nprint()\\nCAT_best_params = clf.best_params_\\nprint(CAT_best_params)\\nprint()\\nprint(\\'F1:\\', clf.best_score_)'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAG353RIJiMR"
      },
      "source": [
        "F1: 0.7601224906430758   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Prs7KYMJiMR"
      },
      "source": [
        "### LightGBM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-1C7qs7JiMR"
      },
      "source": [
        "Закомитил LightGBM model потому что в тренажере обрабатываются больше часа"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJa_hecYJiMS",
        "outputId": "6e7d575d-d9f6-4cdc-bd6b-474095c7daa1"
      },
      "source": [
        "%%time\n",
        "\n",
        "# подбирал параметры кросс валидацией\n",
        "\n",
        "LightGBM_model = LGBMClassifier()\n",
        "hyperparams = [{'max_depth' : [-1], # -1, 1\n",
        "                'learning_rate':[0.1], # 0.03, 0.1, 0.3\n",
        "                'n_estimators' : [1000],  # 200, 500, 1000\n",
        "                'random_state':[12345]}]\n",
        "clf = GridSearchCV(LightGBM_model, hyperparams, scoring='f1',cv=3)\n",
        "clf.fit(train_features, train_target)\n",
        "print(\"Лучшие параметры модели:\")\n",
        "print()\n",
        "LGBM_best_params = clf.best_params_\n",
        "print(LGBM_best_params)\n",
        "print()\n",
        "print('F1:', clf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%time\\n\\n# подбирал параметры кросс валидацией\\n\\nLightGBM_model = LGBMClassifier()\\nhyperparams = [{\\'max_depth\\' : [-1], # -1, 1\\n                \\'learning_rate\\':[0.1], # 0.03, 0.1, 0.3\\n                \\'n_estimators\\' : [1000],  # 200, 500, 1000\\n                \\'random_state\\':[12345]}]\\nclf = GridSearchCV(LightGBM_model, hyperparams, scoring=\\'f1\\',cv=3)\\nclf.fit(train_features, train_target)\\nprint(\"Лучшие параметры модели:\")\\nprint()\\nLGBM_best_params = clf.best_params_\\nprint(LGBM_best_params)\\nprint()\\nprint(\\'F1:\\', clf.best_score_)'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6b6UnMzJiMT"
      },
      "source": [
        "\n",
        "F1: 0.7679683884224614  \n",
        "Wall time: 9min 24s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BN8IvUJiMT"
      },
      "source": [
        "## Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rNLsIYJiMT"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oilF3c1JiMT",
        "outputId": "d71cd15d-92db-4f0c-abfc-77fb728aeb3a"
      },
      "source": [
        "%%time\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.set_params(**LR_best_params)\n",
        "lr_model.fit(train_features, train_target)\n",
        "prediction = lr_model.predict(test_features)\n",
        "f1 = f1_score(test_target, prediction)\n",
        "print('F1 регрессии:', f1)\n",
        "print()\n",
        "print('Матрица ошибок')\n",
        "print(confusion_matrix(test_target, prediction))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 регрессии: 0.7625858123569795\n",
            "\n",
            "Матрица ошибок\n",
            "[[27589  1087]\n",
            " [  573  2666]]\n",
            "\n",
            "CPU times: user 19.8 s, sys: 14.7 s, total: 34.5 s\n",
            "Wall time: 34.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZCkpiCJiMT"
      },
      "source": [
        "### CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x4sDPfTJiMU",
        "outputId": "3f18c5bc-8393-432b-98b5-b64f03680bad"
      },
      "source": [
        "cat_model = LogisticRegression()\n",
        "cat_model.set_params(**CAT_best_params)\n",
        "cat_model.fit(train_features, train_target)\n",
        "prediction = cat_model.predict(test_features)\n",
        "f1 = f1_score(test_target, prediction)\n",
        "print('F1', f1)\n",
        "print()\n",
        "print('Матрица ошибок')\n",
        "print(confusion_matrix(test_target, prediction))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ncat_model = LogisticRegression()\\ncat_model.set_params(**CAT_best_params)\\ncat_model.fit(train_features, train_target)\\nprediction = cat_model.predict(test_features)\\nf1 = f1_score(test_target, prediction)\\nprint('F1', f1)\\nprint()\\nprint('Матрица ошибок')\\nprint(confusion_matrix(test_target, prediction))\\nprint()\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuidU1NMJiMU"
      },
      "source": [
        "F1 CatBoost: 0.7551954913702007\n",
        "\n",
        "Матрица ошибок  \n",
        "[[14191   170]  \n",
        " [  525  1072]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jidZVdQtJiMU"
      },
      "source": [
        "### LightGBM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl3muC7XJiMU",
        "outputId": "0067c44d-e113-474c-bdf2-496c556b3d7d"
      },
      "source": [
        "LightGBM_model = LogisticRegression()\n",
        "LightGBM_model.set_params(**LGBM_best_params)\n",
        "LightGBM_model.fit(train_features, train_target)\n",
        "prediction = LightGBM_model.predict(test_features)\n",
        "f1 = f1_score(test_target, prediction)\n",
        "print('F1:', f1)\n",
        "print()\n",
        "print('Матрица ошибок')\n",
        "print(confusion_matrix(test_target, prediction))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nLightGBM_model = LogisticRegression()\\nLightGBM_model.set_params(**LGBM_best_params)\\nLightGBM_model.fit(train_features, train_target)\\nprediction = LightGBM_model.predict(test_features)\\nf1 = f1_score(test_target, prediction)\\nprint('F1:', f1)\\nprint()\\nprint('Матрица ошибок')\\nprint(confusion_matrix(test_target, prediction))\\nprint()\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEoOa8XLJiMU"
      },
      "source": [
        "\n",
        "F1 регрессии: 0.7600950118764845  \n",
        "\n",
        "Матрица ошибок  \n",
        "[[13730   617]  \n",
        " [  248  1363]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2A7GVfJiMU"
      },
      "source": [
        "**Вывод**\n",
        "Все модели имеют удовлетворительное значение F1. Лучше всех справилась LightGBM model. Но у LogisticRegression меньше ложно отрицательных предсказаний. Значит меньше токсичных коментариев пройдут мимо модерации. То что у LogisticRegression болльше ложно положительных предсказаний не страшно. Так как после модерации они вернуться обратно.\n",
        "\n",
        "Такрим образом, для бизнеса, лучше подходит модель LogisticRegression"
      ]
    }
  ]
}